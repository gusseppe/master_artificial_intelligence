{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OYXOSvo2sVJw"
   },
   "source": [
    "## LAZY LEARNING EXERCISE \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 351
    },
    "colab_type": "code",
    "id": "USM3_DR9sVJy",
    "outputId": "b404a26e-ac21-4aa6-b0b0-7f993141c325"
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import operator\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import glob\n",
    "\n",
    "from urllib import request\n",
    "from scipy.io import arff\n",
    "from io import StringIO\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from tools import eda, all_steps\n",
    "from tools import preprocess as prep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2FiLw-xusVKR"
   },
   "source": [
    "### Function that automatically reads the files and performs the pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7DJhGyMtsVKR"
   },
   "outputs": [],
   "source": [
    "def read_train_test_files():\n",
    "\n",
    "    train_arff_files = glob.glob('../datasets/datasetsCBR/sick/*.train.arff') # for sick dataset\n",
    "#     train_arff_files = glob.glob('../datasets/datasetsCBR/sick/*.train.arff') # for bal dataset\n",
    "#     test_arff_files = glob.glob('../datasets/datasetsCBR/bal/*.test.arff') # for bal dataset\n",
    "    test_arff_files = glob.glob('../datasets/datasetsCBR/sick/*.test.arff') # for sick dataset\n",
    "    \n",
    "\n",
    "    \n",
    "    train_test_split = []\n",
    "    for train_file, test_file in zip(train_arff_files, test_arff_files):\n",
    "        \n",
    "        # Train\n",
    "        df_train = eda.read_arff(path_data=train_file, url_data=None)\n",
    "        X_num_train, X_cat_train, y_train, encoder_train = all_steps.clean_sick(df_train)\n",
    "        X_train = prep.join_features(X_num_train, X_cat_train)\n",
    "\n",
    "        # Test\n",
    "        df_test = eda.read_arff(path_data=test_file, url_data=None)\n",
    "        X_num_test, X_cat_test, y_test, encoder_test = all_steps.clean_sick(df_train, encoder_train)\n",
    "        X_test = prep.join_features(X_num_test, X_cat_test)\n",
    "        \n",
    "\n",
    "        train_test_split.append((X_train.values, y_train.values.reshape(-1, ), \n",
    "                                 X_test.values, y_test.values.reshape(-1, )))\n",
    "    \n",
    "        \n",
    "    return train_test_split\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_test_split = read_train_test_files()\n",
    "len(train_test_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jnWv-DDcsVKT"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3395, 53)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fold_number = 1\n",
    "X_train,y_train, X_test, y_test = train_test_split[fold_number]\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3395,)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reduction techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN provisional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import jit\n",
    "\n",
    "\n",
    "@jit(nopython=True)  \n",
    "def KIblAlgorithm2(X_train, y,  X_test, k, r, voting):\n",
    "\n",
    "#     y_pred = []\n",
    "#     y_pred = np.array([], dtype=np.int)\n",
    "    y_pred = []\n",
    "\n",
    "#     for x in X_test:\n",
    "    len_X = X_test.shape[0]\n",
    "    for index_x in np.arange(0, len_X):\n",
    "#         distances = minskowski(X_train, x, r)\n",
    "        x = X_test[index_x,:]\n",
    "        distances = distance = np.sum(np.abs(X_train-x) ** r, axis=1) ** (1/r)\n",
    "#             print(distances)\n",
    "#         label = get_policy(distances, y_train, k, voting='most')\n",
    "#         y_pred.append(label)\n",
    "        if voting == 'most':\n",
    "            votes = np.zeros(20, dtype=np.int16)\n",
    "    #         votes = dict.fromkeys(range(20),0) # init dict\n",
    "\n",
    "            # find k closet neighbors and vote\n",
    "            # argsort returns the indices that would sort an array\n",
    "            # so indices of nearest neighbors\n",
    "            # we take self.k first\n",
    "            for neighbor_id in np.argsort(distances)[:k]:\n",
    "                # this is a label corresponding to one of the closest neighbor\n",
    "    #                 try:\n",
    "                if len(y) > neighbor_id: # for the case of ib3\n",
    "                    neighbor_label = y[neighbor_id]\n",
    "                else:\n",
    "                    neighbor_label = y[0]\n",
    "            # which updates votes array\n",
    "                votes[neighbor_label] += 1\n",
    "\n",
    "            label = np.argmax(votes)\n",
    "    #         sortcount = sorted(votes.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    #         return sortcount[0][0]\n",
    "\n",
    "        elif voting == 'plurality':\n",
    "\n",
    "\n",
    "            for k in range(k, 0, -1):      \n",
    "    #             votes = dict.fromkeys(range(20),0) # init dict\n",
    "                votes = np.zeros(20, dtype=np.int16)\n",
    "                for neighbor_id in np.argsort(distances)[:k]:\n",
    "    #                     print(self.y)\n",
    "    #                     print(neighbor_id)\n",
    "                    if len(y) > neighbor_id: # for the case of ib3\n",
    "                        neighbor_label = y[neighbor_id]\n",
    "                    else:\n",
    "                        neighbor_label = y[0]\n",
    "\n",
    "                    votes[neighbor_label] += 1\n",
    "\n",
    "    #             sortcount = sorted(votes.items(), key=operator.itemgetter(1), reverse=True)\n",
    "                sortcount = np.sort(votes)[::-1]\n",
    "\n",
    "                if len(sortcount) == 1:\n",
    "    #                 return sortcount[0][0]    \n",
    "                    label = sortcount[0]  \n",
    "\n",
    "                if sortcount[0] == sortcount[1]:\n",
    "                    continue\n",
    "                else:\n",
    "                    label = sortcount[0]\n",
    "            \n",
    "            \n",
    "#         print(label)\n",
    "#         y_pred = np.hstack([y_pred, label])\n",
    "        y_pred.append(label)\n",
    "    \n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "y_pred = KIblAlgorithm2(X_train, y_train, X_test, k=5, r=2, \n",
    "                   voting='most')\n",
    "\n",
    "y_pred[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reduction KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cdist\n",
    "from numba import jit\n",
    "\n",
    "class reductionKIblAlgorithm:\n",
    "    def __init__(self, k=5, r=2, \n",
    "                 voting='most', \n",
    "#                  retention='nr',\n",
    "                 reduction='cnn',\n",
    "                 random_state=42):\n",
    "\n",
    "        self.k = k\n",
    "        self.r = r\n",
    "        self.voting = voting\n",
    "#         self.retention = retention\n",
    "        self.reduction = reduction\n",
    "        self.random_state = random_state\n",
    "        self.knn = KIblAlgorithm2(k, r, voting)\n",
    "#         self.knn = KIblAlgorithm(r=r, k=k, \n",
    "#                     voting_method=voting, \n",
    "#                     retention_type=retention)\n",
    "        self.knn_reduced = self.knn\n",
    "        self.indexes_rm = None\n",
    "        self.X_reduced = None\n",
    "        self.y_reduced = None\n",
    "        \n",
    "    def apply_reduction(self, X, y, \n",
    "                        method='cnn', random_state=42):\n",
    "        \n",
    "        random_instance = np.random.RandomState(random_state)\n",
    "        \n",
    "        def cnn(X_train, y_train):\n",
    "            \"\"\"\n",
    "                Incremental algorithm. Begins with random instances \n",
    "                belonging to each class, then increasing the training \n",
    "                data by inserting those instances that misclassified.\n",
    "            \"\"\"\n",
    "            # Start gathering one instance for each class, randomly.\n",
    "            classes = np.unique(y_train)\n",
    "            indexes_reduced = []\n",
    "            for cl in classes:\n",
    "                y_indexes = np.where(y_train == cl)[0]\n",
    "                index = random_instance.choice(y_indexes, 1)[0]\n",
    "                indexes_reduced.append(index)\n",
    "\n",
    "            x_train_reduced = X_train[indexes_reduced]\n",
    "            y_train_reduced = y_train[indexes_reduced]\n",
    "\n",
    "            for index, (x_instance, y_instance) in enumerate(zip(X_train, y_train)):\n",
    "\n",
    "                best_knn = self.knn\n",
    "                best_knn.fit(x_train_reduced, y_train_reduced)\n",
    "\n",
    "                y_pred_instance = best_knn.predict(np.asarray([x_instance]))\n",
    "          \n",
    "                # If misclassified, add to reduced set.\n",
    "                if y_pred_instance != y_instance:\n",
    "                    x_train_reduced = np.vstack([x_train_reduced, x_instance])\n",
    "                    y_train_reduced = np.hstack([y_train_reduced, y_instance])\n",
    "                    indexes_reduced = np.hstack([indexes_reduced, index])\n",
    "\n",
    "            # Only unique indexes\n",
    "            indexes_reduced = np.unique(indexes_reduced)\n",
    "            \n",
    "            return x_train_reduced, y_train_reduced, indexes_reduced\n",
    "        \n",
    "        def enn(X_train, y_train):\n",
    "            \"\"\"\n",
    "                Non-incremental algorithm. Each instance is removed if it \n",
    "                does not agree with the majority of its knn.\n",
    "            \n",
    "            \"\"\"\n",
    "            classes = np.unique(y_train)\n",
    "            \n",
    "            # Start with all training data.\n",
    "            indexes_reduced = np.arange(len(X_train))\n",
    "\n",
    "            x_train_reduced = X_train\n",
    "            y_train_reduced = y_train\n",
    "\n",
    "            for index, (x_instance, y_instance) in enumerate(zip(X_train, y_train)):\n",
    "\n",
    "                best_knn = self.knn\n",
    "                best_knn.fit(x_train_reduced, y_train_reduced)\n",
    "                y_pred_instance = best_knn.predict(np.asarray([x_instance]))\n",
    "\n",
    "                # If misclassified, remove from the initial set.\n",
    "                if y_pred_instance != y_instance:\n",
    "                    x_train_reduced = np.delete(x_train_reduced, [index], axis=0)\n",
    "                    y_train_reduced = np.delete(y_train_reduced, [index], axis=0)\n",
    "                    indexes_reduced = np.delete(indexes_reduced, [index], axis=0)\n",
    "            \n",
    "            return x_train_reduced, y_train_reduced, indexes_reduced           \n",
    "        \n",
    "\n",
    "        \n",
    "        def ib3(X_train, y_train):\n",
    "            \"\"\"\n",
    "                It is an incremental algorithm. It addresses the IB2's problem \n",
    "                of keeping noisy instances by retaining only acceptable \n",
    "                misclassified instances.\n",
    "            \"\"\"\n",
    "            classes = np.unique(y_train)\n",
    "            \n",
    "            # Start with the first element.\n",
    "#             x_train_reduced = np.reshape(X_train[0], (1, -1))\n",
    "#             y_train_reduced = np.reshape(y_train[0], (1, -1))\n",
    "            x_train_reduced = np.asarray([X_train[0,:]])\n",
    "            y_train_reduced = np.asarray([y_train[0]])\n",
    "        \n",
    "#             print(x_train_reduced)\n",
    "#             print(y_train_reduced)\n",
    "            \n",
    "            acceptable = np.array([0])\n",
    "            \n",
    "            lower = lambda p,z,n: (p + (z**2)/(2*n) - z*((p*(1-p)/n + (z**2)/(4*n**2)))**0.5)/(1 + (z**2)/n)\n",
    "            upper = lambda p,z,n: (p + (z**2)/(2*n) + z*((p*(1-p)/n + (z**2)/(4*n**2)))**0.5)/(1 + (z**2)/n)\n",
    "               \n",
    "            for index, (x_instance, y_instance) in enumerate(zip(X_train, y_train)):\n",
    "\n",
    "                best_knn = self.knn\n",
    "                best_knn.fit(x_train_reduced, y_train_reduced)\n",
    "#                 print(x_train_reduced)\n",
    "#                 print(x_instance)\n",
    "                y_pred_instance = best_knn.predict(np.asarray([x_instance]))\n",
    "#                 y_pred_instance = best_knn.predict(x_instance)\n",
    "\n",
    "                # This part is similar to IB2\n",
    "                if y_pred_instance != y_instance:\n",
    "                    x_train_reduced = np.vstack([x_train_reduced, x_instance])\n",
    "                    acceptable = np.hstack([acceptable, index])\n",
    "                \n",
    "                    \n",
    "                incorrect_class = 0\n",
    "                correct_class = 0\n",
    "                # This part differ from IB2, just acceptable instance are kept.\n",
    "                # Count the number of incorrect and correct classification\n",
    "                for x_instance_reduced in x_train_reduced:\n",
    "                    best_knn = self.knn\n",
    "                    best_knn.fit(x_train_reduced, y_train_reduced)\n",
    "                    y_pred_instance_reduced = best_knn.predict(np.asarray([x_instance_reduced]))\n",
    "                    \n",
    "                    if y_pred_instance_reduced != y_instance:\n",
    "                        incorrect_class += 1\n",
    "                    else:\n",
    "                        correct_class += 1\n",
    "                \n",
    "                n = incorrect_class + correct_class\n",
    "                p = correct_class / n\n",
    "                \n",
    "                # For acceptance\n",
    "                z = 0.9\n",
    "                lower_bound = lower(p, z, n)\n",
    "                upper_bound = upper(p, z, n)\n",
    "#                 print(lower_bound, upper_bound, incorrect_class, correct_class)\n",
    "                if (incorrect_class/n <= lower_bound) or (correct_class/n >= upper_bound):\n",
    "                    acceptable = np.hstack([acceptable, index])\n",
    "                \n",
    "                # For removing\n",
    "                z = 0.7\n",
    "                lower_bound = lower(p, z, n)\n",
    "                upper_bound = upper(p, z, n)\n",
    "                \n",
    "                if (incorrect_class/n <= lower_bound) or (correct_class/n >= upper_bound):\n",
    "                    acceptable = np.delete(acceptable, [index], axis=0)                 \n",
    "\n",
    "            x_train_reduced = X_train[acceptable]\n",
    "            y_train_reduced = y_train[acceptable]\n",
    "            indexes_reduced = acceptable\n",
    "            \n",
    "            return x_train_reduced, y_train_reduced, indexes_reduced    \n",
    "  \n",
    "        def drop1(X_train, y_train):\n",
    "            \"\"\"\n",
    "                This is a non-incremental algorithm. From the paper, \n",
    "                it can be translated as: \"It goes over the dataset in the provided order, \n",
    "                and removes those instances whose removal does not decrease \n",
    "                the accuracy of the 1-NN rule in the remaining dataset\"\n",
    "            \"\"\"\n",
    "            classes = np.unique(y_train)\n",
    "            \n",
    "            # Start with all training data.\n",
    "            indexes_reduced = np.arange(len(X_train))\n",
    "\n",
    "            x_train_reduced = X_train\n",
    "            y_train_reduced = y_train\n",
    "\n",
    "            for index, (x_instance, y_instance) in enumerate(zip(X_train, y_train)):\n",
    "\n",
    "#                 print(index)\n",
    "                best_knn = self.knn\n",
    "                best_knn.fit(x_train_reduced, y_train_reduced)\n",
    "                y_pred_initial = best_knn.predict(x_train_reduced)\n",
    "                \n",
    "                acc_initial = accuracy_score(y_pred_initial, y_train_reduced)\n",
    "                \n",
    "\n",
    "                # Removes one instance from the initial set.\n",
    "                x_train_reduced_t = np.delete(x_train_reduced, [index], axis=0)\n",
    "                y_train_reduced_t = np.delete(y_train_reduced, [index], axis=0)\n",
    "                indexes_reduced_t = np.delete(indexes_reduced, [index], axis=0)\n",
    "                \n",
    "                # Fit again using a 1-nn in the remaining data.\n",
    "                best_1nn = KIblAlgorithm2(k=self.k, r=self.r, \n",
    "                                         voting=self.voting)\n",
    "                \n",
    "                best_1nn.fit(x_train_reduced_t, y_train_reduced_t)\n",
    "                y_pred_1nn_without_instance = best_1nn.predict(x_train_reduced_t)           \n",
    "                \n",
    "                acc_after = accuracy_score(y_pred_1nn_without_instance, y_train_reduced_t)\n",
    "                \n",
    "                # if accuracy after removing the instance is greater than initial, then \n",
    "                # remove from the initial set.\n",
    "                if acc_after >= acc_initial:\n",
    "                    x_train_reduced = np.delete(x_train_reduced, [index], axis=0)\n",
    "                    y_train_reduced = np.delete(y_train_reduced, [index], axis=0)\n",
    "                    indexes_reduced = np.delete(indexes_reduced, [index], axis=0)\n",
    "                \n",
    "            return x_train_reduced, y_train_reduced, indexes_reduced      \n",
    "        \n",
    "        def drop2(X_train, y_train):\n",
    "            \"\"\"\n",
    "                This is a non-incremental algorithm. Similar to DROP1 but \n",
    "                the accuracy of the 1-NN rule is done in the original dataset\".\n",
    "                \n",
    "                Note: A preprocessing is needed before calculating the accuracy:\n",
    "                \"starts with those which are furthest from their nearest \"enemy\" \n",
    "                (two instances are said to be \"enemies\" if they belong to different classes)\".\n",
    "                Hence, this is a partial implementation of drop2.\n",
    "            \"\"\"\n",
    "            classes = np.unique(y_train)\n",
    "            \n",
    "            # Start with all training data.\n",
    "            indexes_reduced = np.arange(len(X_train))\n",
    "\n",
    "            x_train_reduced = X_train\n",
    "            y_train_reduced = y_train\n",
    "            \n",
    "            \n",
    "            best_knn = self.knn\n",
    "            best_knn.fit(X_train, y_train)\n",
    "            y_pred_initial = best_knn.predict(X_train)\n",
    "            acc_initial = accuracy_score(y_pred_initial, y_train)\n",
    "            \n",
    "            for index, (x_instance, y_instance) in enumerate(zip(X_train, y_train)):\n",
    "\n",
    "\n",
    "                # Removes one instance from the initial set.\n",
    "                x_train_reduced_t = np.delete(x_train_reduced, [index], axis=0)\n",
    "                y_train_reduced_t = np.delete(y_train_reduced, [index], axis=0)\n",
    "                indexes_reduced_t = np.delete(indexes_reduced, [index], axis=0)\n",
    "                \n",
    "                # Fit again using a 1-nn in the remaining data.\n",
    "                best_1nn = KIblAlgorithm2(k=self.k, r=self.r, \n",
    "                                         voting=self.voting)\n",
    "                \n",
    "                best_1nn.fit(x_train_reduced_t, y_train_reduced_t)\n",
    "                y_pred_1nn_without_instance = best_1nn.predict(x_train_reduced_t)           \n",
    "                \n",
    "                acc_after = accuracy_score(y_pred_1nn_without_instance, y_train_reduced_t)\n",
    "                \n",
    "                # if accuracy after removing the instance is greater than initial, then \n",
    "                # remove from the initial set.\n",
    "                if acc_after >= acc_initial:\n",
    "                    x_train_reduced = np.delete(x_train_reduced, [index], axis=0)\n",
    "                    y_train_reduced = np.delete(y_train_reduced, [index], axis=0)\n",
    "                    indexes_reduced = np.delete(indexes_reduced, [index], axis=0)\n",
    "                \n",
    "            return x_train_reduced, y_train_reduced, indexes_reduced         \n",
    "        \n",
    "        if method == 'cnn':\n",
    "            return cnn(X, y)\n",
    "        elif method == 'enn':\n",
    "            return enn(X, y)\n",
    "        elif method == 'ib3':\n",
    "            return ib3(X, y)\n",
    "        elif method == 'drop1':\n",
    "            return drop1(X, y)\n",
    "        elif method == 'drop2':\n",
    "            return drop2(X, y)\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "\n",
    "        X_reduced, y_reduced, indexes_rm = self.apply_reduction(X, y, \n",
    "                                             self.reduction,\n",
    "                                             self.random_state)\n",
    "        \n",
    "        self.knn_reduced.fit(X_reduced, y_reduced)\n",
    "        \n",
    "        self.X_reduced = X_reduced\n",
    "        self.y_reduced = y_reduced\n",
    "        self.indexes_rm = indexes_rm\n",
    "        \n",
    "        return self\n",
    "\n",
    "        \n",
    "    def predict(self, X):\n",
    "\n",
    "        y_pred = self.knn_reduced.predict(X)\n",
    "\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing knn vs reduced_knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.99 s, sys: 0 ns, total: 2.99 s\n",
      "Wall time: 3 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9782032400589101"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "knn = KIblAlgorithm2(k=2, r=2, \n",
    "                    voting='most')\n",
    "y_pred = knn.fit(X_train, y_test)\n",
    "\n",
    "y_pred = knn.predict(X_test)\n",
    "accuracy_score(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guess/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:161: DeprecationWarning: in the future out of bounds indices will raise an error instead of being ignored by `numpy.delete`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 30.8 s, sys: 3.54 ms, total: 30.8 s\n",
      "Wall time: 30.8 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9782032400589101"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "reduced_knn = reductionKIblAlgorithm(k=2, r=2, \n",
    "                    voting='most', \n",
    "                    reduction='ib3')\n",
    "reduced_knn.fit(X_train, y_train)\n",
    "# y_pred = reduced_knn.fit(X_train, X_test, y_train, y_test)\n",
    "# accuracy(y_test, y_pred)\n",
    "y_pred_reduced = reduced_knn.predict(X_test)\n",
    "accuracy_score(y_pred_reduced, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## with numba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.92 s, sys: 4 Âµs, total: 4.92 s\n",
      "Wall time: 4.93 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9782032400589101"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "knn = KIblAlgorithm2(k=2, r=2, \n",
    "                    voting='most')\n",
    "y_pred = knn.fit(X_train, y_test)\n",
    "\n",
    "y_pred = knn.predict(X_test)\n",
    "accuracy_score(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "bal_lazy_learning-Copy1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
